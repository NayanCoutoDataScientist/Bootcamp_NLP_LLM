{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 62. Introdução a Transformers",
   "id": "cd261ed487eacf2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> word2vec >> glove >> RNN >> Seq2seq >> Transformers >> Bert >> T5 >> GPT-3 >> GPT-4\n",
    "\n",
    "Transformers:\n",
    "> Modelo Revolucionário\n",
    "> \n",
    "> Utilizado em GPT, T5 e Bert\n",
    "> \n",
    "> Não utiliza recorrência\n",
    "> \n",
    "> Baseado no mecanismo de atenção 'Attention'\n",
    "> \n",
    "> Arquitetura ENDEC\n",
    "\n",
    "Attentio / Self Attention\n",
    "> Aprende a relação contextual entre palavras\n",
    "> \n",
    "> Precisa ser treinado\n",
    "> \n",
    "> Existem modelos pré-treinados\n",
    "\n",
    "ENDEC: Encode / Decode\n",
    "> Encoder: Colocar mensagem em um formato que o computador aprenda\n",
    ">\n",
    "> Decoder: Colocar a mensagem de volta ao formato original\n",
    "\n",
    "1 - Positional Encoding\n",
    "> NLP depende de ordem\n",
    ">\n",
    "> É natural processar em sequência\n",
    ">\n",
    "> A maioria das técnicas processam dados que dependem de Recorrência (RNN)\n",
    ">\n",
    "> Um Transformer submete texto (Embedding) em paralelo\n",
    ">\n",
    "> Performance e melhoras no aprendizagem\n",
    "\n",
    "> Matriz com mesmas dimensões do embedding\n",
    "\n",
    "> Como a matrizPE é calculada?\n",
    ">> Uso da função senoidal\n",
    ">>\n",
    ">> pos é posição da palavra na sentença\n",
    ">>\n",
    ">> i a posição no embedding\n",
    ">>\n",
    ">> Seno quando é par e cosseno quando é impar\n",
    "\n",
    "PE(pos, i) = sin(pos/10000^(i/d_model))\n",
    "PE(pos, i+1) = cos(pos/10000^(2i/d_model))\n",
    "\n",
    "2 - Encoder\n",
    "\n",
    "2.1 - Self Atention\n",
    "> Função de descobrir a relação entre as palavras\n",
    ">\n",
    "> O Transformer calcula cada representação e qual a relação de cada palavra.\n",
    ">\n",
    "> Assim ele compreende que it se refere a street e não ao animal\n",
    ">\n",
    "> O Self-attention é o resultado do processamento\n",
    "\n",
    "2.1.1 Matrizes Q, K, V.\n",
    "> Produz o embedding da sentença gerando a matriz X\n",
    "> \n",
    "> Dimensões de X é o da sentença * tamanho do embedding\n",
    "> \n",
    "> Três novas matrizes ponderadas geradas\n",
    ">> Q: Query Matrix\n",
    ">> K: Key Matrix\n",
    ">> V: Value Matrix\n",
    ">\n",
    "> Pesos são ajustados durante o treinamento\n",
    "\n",
    "2.1.2 - Score: Produto escalar entre Query e Key\n",
    "> Mostra o grau de similaridade entre as palavras\n",
    ">> Query * Key = Value Matrix\n",
    "\n",
    "2.1.3 - Dividir QK pela raiz quadrada das dimensões de Key\n",
    "> Objetivo: Obter bons gradientes\n",
    "QK^t / sqrt(Dx)\n",
    "\n",
    "2.1.4 - Produção da Score Matrix\n",
    "> Normalização entre 0 e 1 com softmax.\n",
    "> \n",
    "> Objetivo é gerar um score de relação entre as palavras\n",
    "Softmax(QK^t / sqrt(Dx)))\n",
    "\n",
    "2.1.5 - Calcular a Matriz Z (matriz de atenção)\n",
    "> Calculo é Score Matrix * Value Matrix\n",
    "> \n",
    "> Cada palava terá um score: Z1, Z2, Z3, Z4\n",
    "> \n",
    "> Mostra o peso da relação entre as palavras\n",
    "\n",
    "Attention: softmax(QK^t / sqrt(Dx)) * V\n",
    "> A matriz Z está pronta para ser enviada através da próxima camada de Feedforward Network (antes de ser normalizada)\n",
    "\n",
    "MultiHead Attention\n",
    "> Em vez de um processo Attention, múltiplos processos são processados e o resultado é somado.\n",
    "> Aumenta a precisão do sistema.\n",
    "> Todas as matrizes são concatenadas e multiplicadas por uma matriz ponderada, pois a RNA espera apenas uma matriz.\n",
    "\n",
    "2.2 - Add & Norm\n",
    "> Camada de normalização\n",
    "> \n",
    "> Objetivo é conectar e normalizar\n",
    "> \n",
    "> Conecta a entrada do multi head attention com sua saída\n",
    "> \n",
    "> Conecta a entrada da feedforward com sua saída\n",
    "\n",
    "2.3 - Feedforward Network\n",
    "> Duas camadas densas com função de ativação ReLU\n",
    "\n",
    "2.4 - Add & Norm"
   ],
   "id": "e6a72e77bf6a64b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 63. Introdução a Transformers Parte II",
   "id": "1b22a5274cfba080"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3 - Decoder\n",
    "> Também são camadas empilhadas\n",
    "> \n",
    "> Também ocorre a conversão para embedding e posicional encodding\n",
    "> \n",
    "> Cada Decoder recebe 2 entradas:\n",
    ">> decoder anterior\n",
    ">>\n",
    ">> saída do encoder\n",
    ">\n",
    "> A conexão com o elemento Multi-Head Attention do decoder\n",
    "\n",
    "3.1.1 - Masked Multi-Head Attention\n",
    "> Ativação de atenção com mascaras nas palavras com tags <sos> e <eos>\n",
    "\n",
    "3.1.2 - Multi-Head Attention\n",
    "\n",
    "3.1.3 - Feedforward Network\n",
    "\n",
    "3.1.4 - Add & Norm\n",
    "\n",
    "4 - Final Layer (Linear e Softmax)\n",
    "> Linear produz um vetor com o tamanho do vocabulário\n",
    "> \n",
    "> Softmax produz as probabilidades\n",
    "> \n",
    "> Decoder gera as palavras com maior probabilidade"
   ],
   "id": "daa5f31b118d0a1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "371d58d88ba5337a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
