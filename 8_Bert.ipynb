{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 64. BERT",
   "id": "4ff0f848ced09754"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BERT: Bidirectional Encoder Representations from Transformers",
   "id": "3527a0d3f55bc099"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Baseado em transformers.\n",
    "> \n",
    "> Possuí apenas a parte do Encode, pois o objetivo é gerar uma representação da linguagem.\n",
    "> \n",
    "> Bidirecional: Capaz de ler o texto em ambas as direções.\n",
    "> \n",
    "> A representação do embedding para cada palavra é contextualizada.\n",
    "> \n",
    "> O contexto é obtido buscando a relação entre as palavras utilizando multi-head attention.\n",
    "> \n",
    "> 24 variações:\n",
    ">> Tiny - 2 * 128\n",
    ">>\n",
    ">> Mini - 4 * 256\n",
    ">>\n",
    ">> Small - 4 * 512\n",
    ">>\n",
    ">> Medium - 8 * 512\n",
    ">>\n",
    ">> Base - 12 * 768\n",
    ">>\n",
    ">> Large - 24 * 1024\n",
    "\n",
    "**Pré Treinamento**\n",
    "> Treinamento não supervisionado.\n",
    ">\n",
    "> Bert em inglês, utiliza Toronto BookCorpus e Wikipédia.\n",
    ">\n",
    "> 4 dias utilizando 16 TPUs.\n",
    ">\n",
    "> Modelos pré-treinados estão disponíveis.\n",
    ">> Masked language modeling\n",
    ">>\n",
    ">> Next sentence prediction\n",
    ">\n",
    "> Pode-se fazer ajuste fino (ajuste de pesos - custo baixo)\n",
    "> \n",
    "> Masked Language Modeling (MLM):\n",
    ">> 15% dos tokes são mascarados, recebendo [MASK].\n",
    ">>\n",
    ">> O modelo tenta prever o token mascarado através de um processo de treinamento (ajuste de pesos) baseado nas palavras não mascaradas.\n",
    ">>\n",
    ">> O modelo retorna uma lista de previsões com probabilidades.\n",
    ">\n",
    "> Next Sentence Prediction (NSP):\n",
    ">> São utilizadas pares de sentenças.\n",
    ">>\n",
    ">> O objetivo é prever se a segunda sentença é a continuação da primeira.\n",
    ">>\n",
    ">> Problema de classificação binário: IsNext, NotNext.\n",
    "\n",
    "**Bert**\n",
    "> O Texto deve ser convertido em 3 embedding layers:\n",
    ">> Token\n",
    ">>\n",
    ">> Segment\n",
    ">>\n",
    ">> Position\n",
    "\n",
    "**Tokenizer**\n",
    "> Se a palavra não estiver presente no vocabulário, é dividida n vezes até que seja encontrada no vocabulário.\n",
    ">\n",
    "> Palavras divididas são sinalizadas com ##\n",
    "\n",
    "**Bert em Português**\n",
    "> O Bert padrão é um modelo em inglês\n",
    "> \n",
    "> Alternativas:\n",
    ">> Multilingual Bert\n",
    ">>> Treinado utilizando a Wikipédia em mais de 100 idiomas\n",
    ">>> Open source\n",
    ">>> Cased e Uncased\n",
    ">>\n",
    ">> BERTimbau\n",
    ">>> Base e Large"
   ],
   "id": "5837f246370d2a8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 65. Variantes de BERT",
   "id": "c47d17fea2dc932d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ALBERT",
   "id": "1054497048b7e53c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Versão mais leve.\n",
    "> \n",
    "> Também possui diferentes versões.\n",
    "> \n",
    "> Menos parâmetros, utilizando técnicas de redução de parâmetros.\n",
    "> \n",
    "> Possui performance superior a vários outros modelos baseados em Transformers."
   ],
   "id": "b393e344eb829f1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## roBERTa",
   "id": "11e4b35900ae8c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Robust Optimized BERT Pretraining Approach.\n",
    "> \n",
    "> Implementado em PyTorch.\n",
    "> \n",
    "> Sem etapa de previsão de próxima sentença.\n",
    "> \n",
    ">  Treinado em diferentes tipos de textos (Notícias, novelas, etc.)."
   ],
   "id": "b9da7384f8a070fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Electra",
   "id": "843c81a1324fcec6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Efficient Learning an Encoder that Classifies Token Replacement Accurately.\n",
    "> \n",
    "> Utiliza replace token detection technique (RTD) ao invés de máscaras.\n",
    "> \n",
    "> RTD: Tokens são substituídos ao invés de mascarados.\n",
    "> \n",
    "> Diferentes versões."
   ],
   "id": "8dfd15c03731e8e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## XLNet",
   "id": "8581b64ea3a1c277"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Generalized Autoregressive Pretraining for Language Understanding.\n",
    "> \n",
    "> Baseado em \"Larger Bidirectional Transformer\": XL.\n",
    "> \n",
    "> Utiliza a técnica de permutação onde os tokens são previstos de forma aleatória."
   ],
   "id": "b6a2f62ce5b1d27e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DistilBERT",
   "id": "59ca491f383a7cc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Versão menor e mais rápida do BERT.\n",
    "> \n",
    "> Desenvolvido pelo Hugging Face.\n",
    "> \n",
    "> Baseado em knowledge distillation.\n",
    ">> Compressão de modelo, onde um modelo menor (estudante) é treinado a partit de um modelo maior (teacher)."
   ],
   "id": "5b1303fe0e1dad3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 66. Hugging Face e OpenAI",
   "id": "c6971fc4c72ff6ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Empresas especializadas em modelos de IA.\n",
    "> \n",
    "> Especialmente NLP.\n",
    "> \n",
    "> OpenAI criado os modelos GPT."
   ],
   "id": "c25fce0285b1c171"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hugging Face",
   "id": "a3d8007e823ffac1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Comunidade de IA - Empresa:\n",
    ">> Modelos.\n",
    ">> Datasets.\n",
    ">> Serviços como Suporte Especializado, AutoNLP...\n",
    "> \n",
    "> Biblioteca Transformers.\n",
    "\n",
    "**Pipeline**\n",
    "> Análise de Sentimento.\n",
    "> \n",
    "> Geração de textos.\n",
    "> \n",
    "> Perguntas e Respostas.\n",
    "> \n",
    "> NER.\n",
    "> \n",
    "> Preencha a lacuna.\n",
    "> \n",
    "> Resumos.\n",
    "> \n",
    "> Tradução."
   ],
   "id": "87365b190db7c784"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OpenAI",
   "id": "afa33f964692b40d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Família de modelos GPT**\n",
    "> Com maior e menor capacidade/custo/performance.\n",
    "> \n",
    "> Método e Modelo Genérico para qualquer tarefa.\n",
    "> \n",
    "> Modelo na Núvem.\n",
    "> \n",
    "> Possível fine-tuning.\n",
    "> \n",
    "> Cobrança por Token.\n",
    "> \n",
    "> Necessita autenticação por chave.\n",
    "\n",
    "https://platform.openai.com/api-keys\n",
    "\n",
    "**API Key**\n",
    "> sk-proj-BguCNViGyQ4Dqzo5BvWiT3BlbkFJneEAQEr78vVTOpjf8Rxg\n",
    "\n"
   ],
   "id": "d7385fc54f004273"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 67. Modelos Pré-treinados",
   "id": "8f873799219a0306"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://huggingface.co/models",
   "id": "fc07782afa76c4ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 68. Aplicação de Perguntas e Respostas",
   "id": "3d17889e2f03127"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T04:50:57.226712Z",
     "start_time": "2024-06-22T04:49:32.099404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "!pip install transformers[tf]\n",
    "!pip install h5py"
   ],
   "id": "c91e5d0899139ded",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers) (3.15.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: transformers[tf] in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (3.15.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (4.66.2)\n",
      "Collecting tensorflow<2.16,>2.9 (from transformers[tf])\n",
      "  Using cached tensorflow-2.15.1-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting onnxconverter-common (from transformers[tf])\n",
      "  Using cached onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting tf2onnx (from transformers[tf])\n",
      "  Using cached tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "INFO: pip is looking at multiple versions of transformers[tf] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers[tf]\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers[tf])\n",
      "  Using cached tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting transformers[tf]\n",
      "  Using cached transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "INFO: pip is still looking at multiple versions of transformers[tf] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "  Using cached transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "  Using cached transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Collecting tensorflow<2.15,>=2.6 (from transformers[tf])\n",
      "  Using cached tensorflow-2.14.1-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting transformers[tf]\n",
      "  Using cached transformers-4.35.1-py3-none-any.whl.metadata (123 kB)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers[tf])\n",
      "  Using cached tokenizers-0.14.1-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting transformers[tf]\n",
      "  Using cached transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "  Using cached transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "  Using cached transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "  Using cached transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[tf])\n",
      "  Using cached tokenizers-0.13.3-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting transformers[tf]\n",
      "  Using cached transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
      "  Using cached transformers-4.33.1-py3-none-any.whl.metadata (119 kB)\n",
      "  Using cached transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n",
      "  Using cached transformers-4.32.1-py3-none-any.whl.metadata (118 kB)\n",
      "Collecting tensorflow<2.14,>=2.6 (from transformers[tf])\n",
      "  Using cached tensorflow-2.13.1-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting transformers[tf]\n",
      "  Using cached transformers-4.32.0-py3-none-any.whl.metadata (118 kB)\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
      "Collecting tensorflow<2.13,>=2.4 (from transformers[tf])\n",
      "  Using cached tensorflow-2.12.1-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting transformers[tf]\n",
      "  Using cached transformers-4.30.1-py3-none-any.whl.metadata (113 kB)\n",
      "  Using cached transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
      "  Using cached transformers-4.29.2-py3-none-any.whl.metadata (112 kB)\n",
      "  Using cached transformers-4.29.1-py3-none-any.whl.metadata (112 kB)\n",
      "  Using cached transformers-4.29.0-py3-none-any.whl.metadata (111 kB)\n",
      "  Using cached transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
      "  Using cached transformers-4.28.0-py3-none-any.whl.metadata (109 kB)\n",
      "  Using cached transformers-4.27.4-py3-none-any.whl.metadata (106 kB)\n",
      "  Using cached transformers-4.27.3-py3-none-any.whl.metadata (106 kB)\n",
      "  Using cached transformers-4.27.2-py3-none-any.whl.metadata (106 kB)\n",
      "  Using cached transformers-4.27.1-py3-none-any.whl.metadata (106 kB)\n",
      "  Using cached transformers-4.27.0-py3-none-any.whl.metadata (106 kB)\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl.metadata (100 kB)\n",
      "  Using cached transformers-4.26.0-py3-none-any.whl.metadata (100 kB)\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl.metadata (93 kB)\n",
      "  Using cached transformers-4.24.0-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: tensorflow>=2.4 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from transformers[tf]) (2.16.1)\n",
      "  Using cached transformers-4.23.1-py3-none-any.whl.metadata (88 kB)\n",
      "  Using cached transformers-4.23.0-py3-none-any.whl.metadata (88 kB)\n",
      "  Using cached transformers-4.22.2-py3-none-any.whl.metadata (84 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers[tf])\n",
      "  Using cached tokenizers-0.12.1.tar.gz (220 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting transformers[tf]\n",
      "  Using cached transformers-4.22.1-py3-none-any.whl.metadata (84 kB)\n",
      "  Using cached transformers-4.22.0-py3-none-any.whl.metadata (84 kB)\n",
      "  Using cached transformers-4.21.3-py3-none-any.whl.metadata (81 kB)\n",
      "  Using cached transformers-4.21.2-py3-none-any.whl.metadata (81 kB)\n",
      "  Using cached transformers-4.21.1-py3-none-any.whl.metadata (81 kB)\n",
      "  Using cached transformers-4.21.0-py3-none-any.whl.metadata (81 kB)\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[tf]) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[tf]) (4.9.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow>=2.4->transformers[tf]) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (0.3.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (0.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from requests->transformers[tf]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from requests->transformers[tf]) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from requests->transformers[tf]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from requests->transformers[tf]) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tqdm>=4.27->transformers[tf]) (0.4.6)\n",
      "Collecting onnx (from onnxconverter-common->transformers[tf])\n",
      "  Using cached onnx-1.16.1-cp311-cp311-win_amd64.whl.metadata (16 kB)\n",
      "INFO: pip is looking at multiple versions of onnxconverter-common to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting onnxconverter-common (from transformers[tf])\n",
      "  Using cached onnxconverter_common-1.13.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf])\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (0.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow>=2.4->transformers[tf]) (0.1.0)\n",
      "Using cached onnxconverter_common-1.13.0-py2.py3-none-any.whl (83 kB)\n",
      "Using cached tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
      "Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Using cached onnx-1.16.1-cp311-cp311-win_amd64.whl (14.4 MB)\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [51 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\nayan couto\\.conda\\envs\\poo\\lib\\site-packages (from h5py) (1.26.4)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T04:52:51.846967Z",
     "start_time": "2024-06-22T04:52:31.588015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "from transformers import pipeline"
   ],
   "id": "957b031b1fb6fe1e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T05:02:26.616728Z",
     "start_time": "2024-06-22T05:01:08.247571Z"
    }
   },
   "cell_type": "code",
   "source": "qea = pipeline(\"question-answering\", model=\"timpal0l/mdeberta-v3-base-squad2\")",
   "id": "e46319e1e22c0c30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84b2b326bb044717a88fb2a65c663a4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nayan Couto\\.conda\\envs\\POO\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDebertaV2ForQuestionAnswering: ['deberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFDebertaV2ForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDebertaV2ForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDebertaV2ForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaV2ForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "641651cf5d9f415cb0fbfc8deccb4907"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "931a08121692493ba06a3fffc8bc58ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58e8270cb46e4f589ed7e415a7711e90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe44aab6e6b04e42b1c60de73fe62f01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T05:09:57.290400Z",
     "start_time": "2024-06-22T05:09:57.283465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texto =  'Carl Edward Sagan foi um cientista planetário, astrônomo, astrobiólogo, astrofísico, escritor, divulgador científico e ativista norte-americano.'\n",
    "#pergunta = 'Quais as profissões de Sagan?'\n",
    "pergunta = 'Quem é Carl Sagan?'"
   ],
   "id": "11cce590118e39d2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T05:09:59.286801Z",
     "start_time": "2024-06-22T05:09:58.069255Z"
    }
   },
   "cell_type": "code",
   "source": "resposta = qea(question=pergunta, context=texto)",
   "id": "95da7df2563375b1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T05:10:04.332668Z",
     "start_time": "2024-06-22T05:10:04.325484Z"
    }
   },
   "cell_type": "code",
   "source": "print(pergunta)",
   "id": "56293463f15c35e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quem é Carl Sagan?\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T05:10:05.191958Z",
     "start_time": "2024-06-22T05:10:05.182148Z"
    }
   },
   "cell_type": "code",
   "source": "print(resposta['answer'])",
   "id": "f681ba5913c8d49f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " escritor, divulgador científico e ativista norte-americano.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T05:10:07.248860Z",
     "start_time": "2024-06-22T05:10:07.240825Z"
    }
   },
   "cell_type": "code",
   "source": "print(resposta['score'])",
   "id": "e1b7ea3a90692d2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0612836629152298\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bd8afc97a016d374"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
