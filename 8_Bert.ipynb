{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 64. BERT",
   "id": "4ff0f848ced09754"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BERT: Bidirectional Encoder Representations from Transformers",
   "id": "3527a0d3f55bc099"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Baseado em transformers.\n",
    "> \n",
    "> Possuí apenas a parte do Encode, pois o objetivo é gerar uma representação da linguagem.\n",
    "> \n",
    "> Bidirecional: Capaz de ler o texto em ambas as direções.\n",
    "> \n",
    "> A representação do embedding para cada palavra é contextualizada.\n",
    "> \n",
    "> O contexto é obtido buscando a relação entre as palavras utilizando multi-head attention.\n",
    "> \n",
    "> 24 variações:\n",
    ">> Tiny - 2 * 128\n",
    ">>\n",
    ">> Mini - 4 * 256\n",
    ">>\n",
    ">> Small - 4 * 512\n",
    ">>\n",
    ">> Medium - 8 * 512\n",
    ">>\n",
    ">> Base - 12 * 768\n",
    ">>\n",
    ">> Large - 24 * 1024\n",
    "\n",
    "**Pré Treinamento**\n",
    "> Treinamento não supervisionado.\n",
    ">\n",
    "> Bert em inglês, utiliza Toronto BookCorpus e Wikipédia.\n",
    ">\n",
    "> 4 dias utilizando 16 TPUs.\n",
    ">\n",
    "> Modelos pré-treinados estão disponíveis.\n",
    ">> Masked language modeling\n",
    ">>\n",
    ">> Next sentence prediction\n",
    ">\n",
    "> Pode-se fazer ajuste fino (ajuste de pesos - custo baixo)\n",
    "> \n",
    "> Masked Language Modeling (MLM):\n",
    ">> 15% dos tokes são mascarados, recebendo [MASK].\n",
    ">>\n",
    ">> O modelo tenta prever o token mascarado através de um processo de treinamento (ajuste de pesos) baseado nas palavras não mascaradas.\n",
    ">>\n",
    ">> O modelo retorna uma lista de previsões com probabilidades.\n",
    ">\n",
    "> Next Sentence Prediction (NSP):\n",
    ">> São utilizadas pares de sentenças.\n",
    ">>\n",
    ">> O objetivo é prever se a segunda sentença é a continuação da primeira.\n",
    ">>\n",
    ">> Problema de classificação binário: IsNext, NotNext.\n",
    "\n",
    "**Bert**\n",
    "> O Texto deve ser convertido em 3 embedding layers:\n",
    ">> Token\n",
    ">>\n",
    ">> Segment\n",
    ">>\n",
    ">> Position\n",
    "\n",
    "**Tokenizer**\n",
    "> Se a palavra não estiver presente no vocabulário, é dividida n vezes até que seja encontrada no vocabulário.\n",
    ">\n",
    "> Palavras divididas são sinalizadas com ##\n",
    "\n",
    "**Bert em Português**\n",
    "> O Bert padrão é um modelo em inglês\n",
    "> \n",
    "> Alternativas:\n",
    ">> Multilingual Bert\n",
    ">>> Treinado utilizando a Wikipédia em mais de 100 idiomas\n",
    ">>> Open source\n",
    ">>> Cased e Uncased\n",
    ">>\n",
    ">> BERTimbau\n",
    ">>> Base e Large"
   ],
   "id": "5837f246370d2a8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 65. Variantes de BERT",
   "id": "c47d17fea2dc932d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ALBERT",
   "id": "1054497048b7e53c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Versão mais leve.\n",
    "> \n",
    "> Também possui diferentes versões.\n",
    "> \n",
    "> Menos parâmetros, utilizando técnicas de redução de parâmetros.\n",
    "> \n",
    "> Possui performance superior a vários outros modelos baseados em Transformers."
   ],
   "id": "b393e344eb829f1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## roBERTa",
   "id": "11e4b35900ae8c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Robust Optimized BERT Pretraining Approach.\n",
    "> \n",
    "> Implementado em PyTorch.\n",
    "> \n",
    "> Sem etapa de previsão de próxima sentença.\n",
    "> \n",
    ">  Treinado em diferentes tipos de textos (Notícias, novelas, etc.)."
   ],
   "id": "b9da7384f8a070fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Electra",
   "id": "843c81a1324fcec6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Efficient Learning an Encoder that Classifies Token Replacement Accurately.\n",
    "> \n",
    "> Utiliza replace token detection technique (RTD) ao invés de máscaras.\n",
    "> \n",
    "> RTD: Tokens são substituídos ao invés de mascarados.\n",
    "> \n",
    "> Diferentes versões."
   ],
   "id": "8dfd15c03731e8e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## XLNet",
   "id": "8581b64ea3a1c277"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Generalized Autoregressive Pretraining for Language Understanding.\n",
    "> \n",
    "> Baseado em \"Larger Bidirectional Transformer\": XL.\n",
    "> \n",
    "> Utiliza a técnica de permutação onde os tokens são previstos de forma aleatória."
   ],
   "id": "b6a2f62ce5b1d27e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DistilBERT",
   "id": "59ca491f383a7cc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Versão menor e mais rápida do BERT.\n",
    "> \n",
    "> Desenvolvido pelo Hugging Face.\n",
    "> \n",
    "> Baseado em knowledge distillation.\n",
    ">> Compressão de modelo, onde um modelo menor (estudante) é treinado a partit de um modelo maior (teacher)."
   ],
   "id": "5b1303fe0e1dad3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 66. Hugging Face e OpenAI",
   "id": "c6971fc4c72ff6ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Empresas especializadas em modelos de IA.\n",
    "> \n",
    "> Especialmente NLP.\n",
    "> \n",
    "> OpenAI criado os modelos GPT."
   ],
   "id": "c25fce0285b1c171"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hugging Face",
   "id": "a3d8007e823ffac1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Comunidade de IA - Empresa:\n",
    ">> Modelos.\n",
    ">> Datasets.\n",
    ">> Serviços como Suporte Especializado, AutoNLP...\n",
    "> \n",
    "> Biblioteca Transformers.\n",
    "\n",
    "**Pipeline**\n",
    "> Análise de Sentimento.\n",
    "> \n",
    "> Geração de textos.\n",
    "> \n",
    "> Perguntas e Respostas.\n",
    "> \n",
    "> NER.\n",
    "> \n",
    "> Preencha a lacuna.\n",
    "> \n",
    "> Resumos.\n",
    "> \n",
    "> Tradução."
   ],
   "id": "87365b190db7c784"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OpenAI",
   "id": "afa33f964692b40d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Família de modelos GPT**\n",
    "> Com maior e menor capacidade/custo/performance.\n",
    "> \n",
    "> Método e Modelo Genérico para qualquer tarefa.\n",
    "> \n",
    "> Modelo na Núvem.\n",
    "> \n",
    "> Possível fine-tuning.\n",
    "> \n",
    "> Cobrança por Token.\n",
    "> \n",
    "> Necessita autenticação por chave.\n",
    "\n",
    "https://platform.openai.com/api-keys\n",
    "\n",
    "**API Key**\n",
    "> sk-proj-BguCNViGyQ4Dqzo5BvWiT3BlbkFJneEAQEr78vVTOpjf8Rxg\n",
    "\n"
   ],
   "id": "d7385fc54f004273"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f5e8ce95f43bdd2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
